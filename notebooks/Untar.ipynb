{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import java.io.{BufferedReader, InputStreamReader}\n",
        "\n",
        "import org.apache.commons.compress.archivers.tar.TarArchiveInputStream\n",
        "import org.apache.commons.compress.compressors.gzip.GzipCompressorInputStream\n",
        "import org.apache.spark.input.PortableDataStream"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.slf4j.LoggerFactory\n",
        "val logger = LoggerFactory getLogger getClass.getName"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "logger = org.slf4j.impl.Log4jLoggerAdapter($iw)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "org.slf4j.impl.Log4jLoggerAdapter($iw)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val fileNameFilter = Some(\".*.json$\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "fileNameFilter = Some(.*.json$)\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "Some(.*.json$)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val tarRDD = sc.binaryFiles(\"yelp-dataset-challenge/importer/src/test/resources/sample_yelp_dataset.tar.gz\")"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tarRDD = yelp-dataset-challenge/importer/src/test/resources/sample_yelp_dataset.tar.gz BinaryFileRDD[3] at binaryFiles at <console>:36\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "lastException: Throwable = null\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "yelp-dataset-challenge/importer/src/test/resources/sample_yelp_dataset.tar.gz BinaryFileRDD[3] at binaryFiles at <console>:36"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tarRDD.flatMap {\n",
        "    case (name: String, content: PortableDataStream) =>\n",
        "        val tar = new TarArchiveInputStream(new GzipCompressorInputStream(content.open))\n",
        "        Stream.continually(tar.getNextEntry)\n",
        "            .takeWhile {\n",
        "              case null => tar.close(); false\n",
        "              case _ => true\n",
        "            }\n",
        "            .filter(!_.isDirectory)\n",
        "            .filter(_.getSize > 0)\n",
        "            .filter(entry => fileNameFilter.forall(entry.getName.matches(_)))\n",
        "            .map(_.getName -> new BufferedReader(new InputStreamReader(tar)))\n",
        "}\n",
        ".collect()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "lastException = null\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": [
              "Name: org.apache.spark.SparkException\n",
              "Message: Job aborted due to stage failure: Task 0.0 in stage 1.0 (TID 1) had a not serializable result: java.io.BufferedReader\n",
              "Serialization stack:\n",
              "\t- object not serializable (class: java.io.BufferedReader, value: java.io.BufferedReader@4d9191af)\n",
              "\t- field (class: scala.Tuple2, name: _2, type: class java.lang.Object)\n",
              "\t- object (class scala.Tuple2, (business.json,java.io.BufferedReader@4d9191af))\n",
              "\t- element of array (index: 0)\n",
              "\t- array (class [Lscala.Tuple2;, size 6)\n",
              "StackTrace: Serialization stack:\n",
              "\t- object not serializable (class: java.io.BufferedReader, value: java.io.BufferedReader@4d9191af)\n",
              "\t- field (class: scala.Tuple2, name: _2, type: class java.lang.Object)\n",
              "\t- object (class scala.Tuple2, (business.json,java.io.BufferedReader@4d9191af))\n",
              "\t- element of array (index: 0)\n",
              "\t- array (class [Lscala.Tuple2;, size 6)\n",
              "  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n",
              "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n",
              "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n",
              "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
              "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
              "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n",
              "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
              "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n",
              "  at scala.Option.foreach(Option.scala:257)\n",
              "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n",
              "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n",
              "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n",
              "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n",
              "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
              "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n",
              "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
              "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
              "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
              "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
              "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n",
              "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
              "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
              "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
              "  at org.apache.spark.rdd.RDD.collect(RDD.scala:944)"
            ]
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "apache_toree_scala"
    },
    "language_info": {
      "name": "scala",
      "version": "2.11.12",
      "file_extension": ".scala",
      "pygments_lexer": "scala",
      "mimetype": "text/x-scala",
      "codemirror_mode": "text/x-scala"
    },
    "kernelspec": {
      "name": "apache_toree_scala",
      "language": "scala",
      "display_name": "Apache Toree - Scala"
    },
    "nteract": {
      "version": "0.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}